import torch
import torch.nn as nn
import torch.nn.functional as F

class DicomClassifier(nn.Module):
    def __init__(self,input_size=3328):
        super(DicomClassifier, self).__init__()
        self.input_size = input_size
        self.lconv1 = nn.Conv2d(input_size, 32, 2, padding=1)
        self.lbn1 = nn.BatchNorm2d(32)
        self.lconv2 = nn.Conv2d(32, 64, 2,padding=1)
        self.lbn2 = nn.BatchNorm2d(64)
        self.lconv3 = nn.Conv2d(64, 128, 2,padding=1)
        self.lbn3 = nn.BatchNorm2d(128)
        self.lflatten = nn.Flatten()
        self.lfc1 = nn.Linear(128*8*8, 128)
        self.lbnfc1 = nn.BatchNorm1d(128)
        self.lfc2 = nn.Linear(128, 64)
        self.lbnfc2 = nn.BatchNorm1d(64)
        self.ldropout = nn.Dropout(0.5)
        self.lhead = nn.Linear(64, 1)
        self.rconv1 = nn.Conv2d(input_size, 32, 2, padding=1)
        self.rbn1 = nn.BatchNorm2d(32)
        self.rconv2 = nn.Conv2d(32, 64, 2, padding=1)
        self.rbn2 = nn.BatchNorm2d(64)
        self.rconv3 = nn.Conv2d(64, 128, 2, padding=1)
        self.rbn3 = nn.BatchNorm2d(128)
        self.rflatten = nn.Flatten()
        self.rfc1 = nn.Linear(128*8*8, 128)
        self.rbnfc1 = nn.BatchNorm1d(128)
        self.rfc2 = nn.Linear(128, 64)
        self.rbnfc2 = nn.BatchNorm1d(64)
        self.rdropout = nn.Dropout(0.5)
        self.rhead = nn.Linear(64, 1)

    def forward(self,x):
        lconv1 = F.relu(self.lbn1(self.lconv1(x[0])))
        lconv2 = F.relu(self.lbn2(self.lconv2(lconv1)))
        lconv3 = F.relu(self.lbn3(self.lconv3(lconv2)))
        lflat = self.flatten(lconv3)
        lfc1 = F.relu(self.lbnfc1(self.lfc1(lflat)))
        lfc2 = F.relu(self.lbnfc2(self.lfc2(lfc1)))
        lfc3 = F.relu(self.ldropout(lfc2))
        rconv1 = F.relu(self.rbn1(self.rconv1(x[1])))
        rconv2 = F.relu(self.rbn2(self.rconv2(rconv1)))
        rconv3 = F.relu(self.rbn3(self.rconv3(rconv2)))
        rflat = self.flatten(rconv3)
        rfc1 = F.relu(self.rbnfc1(self.lfc1(rflat)))
        rfc2 = F.relu(self.rbnfc2(self.lfc2(rfc1)))
        rfc3 = F.relu(self.rdropout(rfc2))
        return self.lhead(lfc3),self.rhead(rfc3)